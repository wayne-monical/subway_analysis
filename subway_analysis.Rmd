---
title: "Subway Analysis"
author: "Wayne Monical"
date: "2024-10-01"
output: github_document
---


```{r, results = 'hide'}
library(tidyverse)
library(readxl)
```




## Subway Data

### Data Cleaning

We begin by importing and cleaning the subway data set. This data set contains information on the organization, location, and amenities in the New York city subway system. The data is tidy. The data is consistent, i.e. each row of the data frame contains information on one subway entrance or exit. It is structured, i.e. every variable, such as station, line, and latitude, has its own column. Finally, all values have their own cell, i.e. there is no value concatenation. 

```{r}
subway = read_csv(
  "data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv") |> 
  janitor::clean_names()
```


```{r}
subway_cols = c("line", "station_name", "station_latitude", "station_longitude",
                   "route1", "route2", "route3", "route4", "route5", "route6", "route7", 
                   "route8", "route9", "route10", "route11", "entrance_type", "entry",
                   "exit_only", "vending", "ada",    "ada_notes", "entrance_latitude",
                   "entrance_longitude")

subway = subway |> select(all_of(subway_cols))
```

We see here that `entry`, `exit_only`, and `vending` are character vectors, and `ada` is a logical vector.
```{r}
logical_vars = c('entry', 'exit_only', 'vending', 'ada')

subway |> 
  select(all_of(logical_vars)) |> 
  summary()
```

Inspecting the unique values of each character, we can find the character strings that correspond to each logical value.
```{r}
char_vars = c('entry', 'exit_only', 'vending')

for(var in char_vars){
  print(var)
  
  subway |> 
    pull(var) |> 
    unique() |> 
    print()
}
```

Using the `mutate()` function, we can reassign these variables to be logical based on the character strings that we found.
```{r}
subway = subway |> 
  mutate(
    entry = entry == 'YES',
    exit_only = !is.na(exit_only),
    vending = vending == 'YES')
```


There are 465 distinct subway stations.
```{r}
subway |> select('station_name', 'line') |> distinct() |> nrow()
```

We must also standardize the route variables as characters. 
```{r}
subway = subway |> 
  mutate(
    route1 = as.character(route1),
    route2 = as.character(route2),
    route3 = as.character(route3),
    route4 = as.character(route4),
    route5 = as.character(route5),
    route6 = as.character(route6),
    route7 = as.character(route7),
    route8 = as.character(route8),
    route9 = as.character(route9),
    route10 = as.character(route10),
    route11 = as.character(route11),
  )
```


There are 84 ADA compliant subway stations. 
```{r}
subway |> filter(ada) |> select('station_name', 'line') |> distinct() |> nrow()
```

73% of station entrances and exits without vending allow entrance. 
```{r}
subway |> filter(!vending) |> pull(exit_only) |> summary()

133 / (50 + 133)
```

### Pivoting 
 
 Here, we pivot the route variables to a longer format and drop the rows that do not have a route. 
```{r}
subway= subway |> 
  pivot_longer(
    route1:route11,
    names_to = 'route_number',
    values_to = 'route'
  ) |> 
  filter(!is.na(route))
```

Using the code below, we find that 17 of the 43 of the stations on the A, 39%, are ADA compliant.  
```{r}
subway |> 
  filter(route == 'A') |> 
  select('line', 'station_name', 'ada') |>
  distinct() |> 
  pull(ada) |>
  summary()

107 / (166 + 107)
```

## Mr Trash Wheel

Mr Trash Wheel is a machine that picks trash from the water with a large wheel. We will now analyze how much trash and what kind of trash he picks. We begin by reading in the 2024 Mr Trash Wheel sheet using the `read_excel` function. We have specified the excel sheet and the data range, and we have rounded the number of sports balls and set it to integer values. We also transform the year variable into an integer, so that when we combine this data set with Gwynnda Trash Wheel's data set, we are able to smoothly bind the rows. We have omitted the totaling rows from the original excel sheet, since they do not contain dumpster-specific data. 
```{r}
mr_trash = 
  read_excel(
    path = 'data/202409 Trash Wheel Collection Data.xlsx',
    sheet = 'Mr. Trash Wheel',
    range = 'A2:N653') |> 
  janitor::clean_names() |> 
  mutate(
    sports_balls = sports_balls |> round() |> as.integer(),
    year = year |> as.integer())
```




Using the same code structure, we can import and clean Gwynnda Trash Wheel's data to create a single tidy data set. This data set is tidy because each row corresponds to the data from a single dumpster, each column corresponds to a single variable, and each cell contains a single value. 
```{r}
gwyndda_trash = 
  read_excel(
    path = 'data/202409 Trash Wheel Collection Data.xlsx',
    sheet = 'Gwynnda Trash Wheel',
    range = 'A2:K265') |> 
  janitor::clean_names()
```

Adding a variable for `trash_wheel`, we can combine Mr Trash Wheel's data with Gwynnda's data with the `bind_rows` function, stacking the rows on top of each other. Note that there are some differences between the two data sets. For example, Gwynnda Trash Wheel has no `sports_balls` variable, so this variable is `NA` in Gwynnda's rows. 

```{r}
mr_trash = mr_trash |> mutate(trash_wheel = 'mr_trash')

gwyndda_trash = gwyndda_trash |> mutate(trash_wheel = 'gwynnda')

trash = bind_rows(mr_trash, gwyndda_trash)
```


With this tidy data set, we can use `dyplr` functions to answer any concrete question about the data. For example, from `r trash |> filter(trash_wheel == 'mr_trash') |> pull(year) |> min()` to `r trash |> filter(trash_wheel == 'mr_trash') |> pull(year) |> max()`, Mr. Trash Wheel collected a total of `r trash |> filter(trash_wheel == 'mr_trash') |> pull(weight_tons) |> sum() |> round()` tons of trash.

```{r}
trash |> filter(trash_wheel == 'mr_trash') |> pull(weight_tons) |> sum()
```

By filtering for bins collected by Gwynnda in June of 2022, we can find that she collected a total of `r trash |> 
  filter(
    trash_wheel == 'gwynnda',
    year == 2022,
    month == 'June') |> 
  pull(cigarette_butts) |> 
  sum()` cigarette butts in that month. 
```{r}
trash |> 
  filter(
    trash_wheel == 'gwynnda',
    year == 2022,
    month == 'June') |> 
  pull(cigarette_butts) |> sum()
```



## Great British Baking Show

### Data Carpentry

The first step is to import the data and look at it. Any non-tidy data can be corrected at this stage. 

1) The `bakers` and `bakes` data is structured as expected, but the `results` data is not. Examining the csv file, we can see that the data frame begins on the third row. Specifying the argument `skip = 2` instructs the `read_csv()` function to skip the first two rows of the csv file, and therefore read the data as we expect. 

2) The data will be joined by the first names of the bakers, their series, and their episode. Therefore we will create the variable for first name in the `bakers` data frame by extracting the first word from the `baker_name` variable.

3) The `bakes` data frame has the column `signature_bakes` with the value "N/A", which should be treated as `NA`. We can specify this as a missing value in the `reaed_csv` function.
```{r}
bakers =
  read_csv('data/gbb_datasets/bakers.csv') |> 
  janitor::clean_names() |> 
  mutate(baker = stringr::word(baker_name, 1))

bakes =
  read_csv(
    'data/gbb_datasets/bakes.csv',
    na = c('N/A')) |> 
  janitor::clean_names()

results =
  read_csv('data/gbb_datasets/results.csv', skip = 2) |> 
  janitor::clean_names()
```


The second step is to check for completeness. We use the code below to search for any discrepancy between the `bakers` and the `results` data frame. We find that there is one missing name in each data set, namely "Jo" and "Joanne". Upon further research, we find that both of these names occur in series 2, and are very likely the same person. 
```{r}
anti_join(bakers, results) |> pull(baker) |> unique()
anti_join(results, bakers) |> pull(baker) |> unique()
```
Here we change all instances of "Jo" to Joanne. So now the `bakers` and `results` data frames agree. 
```{r}
bakers = 
  bakers |> 
  mutate(baker = ifelse(baker == "Jo", "Joanne", baker))

anti_join(bakers, results) |> pull(baker) |> unique()
anti_join(results, bakers) |> pull(baker) |> unique()
```

We find and correct the same Joanne. issue in the `bakes` data set
```{r}
anti_join(bakes, bakers) |> pull(baker) |> unique()

bakes = 
  bakes |> 
  mutate(baker = ifelse(baker == "\"Jo\"", "Joanne", baker))
```


However, there are still 24 names that are missing from `bakes`, all of which are from series 9 and 10. Inspecting `bakes`, we see that the max series it has is up until season 8, explaining the difference. 
```{r}
anti_join(bakes, bakers) |> pull(baker) |> unique()
anti_join(bakers, bakes) |> 
  select(series, baker) |> 
  arrange(series, baker)

bakes |> pull(series) |> max()
```

### Merging Data

Here we merge the data with left joins, since there are missing data points from `bakes`. In the code output, we see that we have successfully joined on our intended columns. The data set is tidy. Each row represents an individual contestants performance on a single episode of a single series. If a contestant does not compete in an episode since they are eliminated, their `result` variable is `NA`, so we may also drop these rows. We also reorder the rows and columns for clarity. Finally, we save this data frame as a csv file.

```{r}
gbb = 
  bakers |> 
  left_join(results) |> 
  left_join(bakes) |> 
  filter(!is.na(result)) |> 
  select(series, baker_name, baker, baker_age, baker_occupation, 
         hometown, episode, result, technical, signature_bake, 
         show_stopper) |> 
  arrange(series, baker_name, episode, result)

gbb |> write.csv('data/gbb_datasets/great_british_bake_off.csv')
```


### Star Baker Table

In the code below, we filter for episodes in seasons 5 through 10 and the name of the star baker in each episode. I included the winner of the season in this table, because the season winner is the star baker of the final episode in each season. I arranged the table in order of series and then episode. In season five, we see that Richard Burr was star baker in five of the ten episodes, but failed to secure the win at the end of the season, which is surprising. 

```{r}
gbb |> 
  filter(
  series >= 5,
  series <= 10,
  result %in% c('STAR BAKER', 'WINNER')
  ) |> 
  select(
    series, baker_name, episode, result
  ) |> 
  arrange(series, episode)
```


### Viewership

Here we import, clean, and analyze the viewership data from the Great British Bake Off. We see that the data is structured in the wide format, with each row containing all ratings of each episode number across ten seasons. In order to make this data tidy we will pivot the data so that each row is a single viewership number of a single episode. We will drop the missing values, since they correspond to non-existent episodes in the first several seasons. We will arrange the rows and columns for clarity.
```{r}
viewers =
  read_csv('data/gbb_datasets/viewers.csv') |> 
  janitor::clean_names() |> 
  pivot_longer(
    series_1:series_10,
    names_to = 'series',
    values_to = 'viewership'
  ) |> drop_na() |> 
  select(series, episode, viewership) |> 
  arrange(series, episode, viewership)
```

Here are the first ten rows of the `viewer` dataframe. 
```{r}
viewers |> head(10)
```
The average viewership by season is given below. 
```{r}
viewers |> 
  group_by(series) |> 
  summarise(mean_views = round(mean(viewership), 2))
```

